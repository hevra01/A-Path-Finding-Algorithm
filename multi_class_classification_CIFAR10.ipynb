{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_class_classification_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORT2SDl7XUikod6gUsE7jv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca29733e785041f9b9dc1d84dbdd49f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e288d43a77e4b74a6bebc35e1772a5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0331f58551204393b05233a5cb41a6d4",
              "IPY_MODEL_28d495ff6ac94518a205d8bc7f28a0dc",
              "IPY_MODEL_9ffbb876e5024fab90ea27ddc3f52cd0"
            ]
          }
        },
        "2e288d43a77e4b74a6bebc35e1772a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0331f58551204393b05233a5cb41a6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02d025fe34244480a02d3e518924f2c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02e677f1a3d54fddb565e4dbfa42b976"
          }
        },
        "28d495ff6ac94518a205d8bc7f28a0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9682cd76f834b42a159b6c6543f1628",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd3f49bf001f467b88204f536de8c62d"
          }
        },
        "9ffbb876e5024fab90ea27ddc3f52cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aaa7ce9ce94b4216b78d6811b9e81487",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:04&lt;00:00, 42117087.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b64166d2c2341b9a7cd6facac064a85"
          }
        },
        "02d025fe34244480a02d3e518924f2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02e677f1a3d54fddb565e4dbfa42b976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9682cd76f834b42a159b6c6543f1628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd3f49bf001f467b88204f536de8c62d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aaa7ce9ce94b4216b78d6811b9e81487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b64166d2c2341b9a7cd6facac064a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hevra01/A-Path-Finding-Algorithm/blob/main/multi_class_classification_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZoJuyU9gf_X"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "torch.manual_seed(0) \n",
        "import torch.nn as nn # more object oriented\n",
        "import torch.nn.functional as F # more functional\n",
        "# torchvision is used for image and video transformations. It also has its own datasets.\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from tqdm import tqdm\n",
        "import ssl\n",
        "import torch.optim as optim\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "ca29733e785041f9b9dc1d84dbdd49f8",
            "2e288d43a77e4b74a6bebc35e1772a5b",
            "0331f58551204393b05233a5cb41a6d4",
            "28d495ff6ac94518a205d8bc7f28a0dc",
            "9ffbb876e5024fab90ea27ddc3f52cd0",
            "02d025fe34244480a02d3e518924f2c9",
            "02e677f1a3d54fddb565e4dbfa42b976",
            "a9682cd76f834b42a159b6c6543f1628",
            "cd3f49bf001f467b88204f536de8c62d",
            "aaa7ce9ce94b4216b78d6811b9e81487",
            "9b64166d2c2341b9a7cd6facac064a85"
          ]
        },
        "id": "30iYkn4ogmj9",
        "outputId": "feec3d45-6c0a-4c80-feff-3997c57d6c6d"
      },
      "source": [
        "from torchvision.datasets import CIFAR10 # torchvision has its own datasets so we can import from there directly\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# preprocessing (normalization) of the training data\n",
        "train_transform = T. Compose ([\n",
        "# can add additional transforms on images\n",
        "T. ToTensor () , # convert images to PyTorch tensors, which are arrays\n",
        "T. Grayscale () , # RGB to grayscale\n",
        "T. Normalize ( mean =(0.5 ,) , std=(0.5 ,) ) # normalization\n",
        "# speeds up the convergence\n",
        "# and improves the accuracy\n",
        "])\n",
        "\n",
        "# preprocessing (normalization) of the testing data\n",
        "val_transform = test_transform = T. Compose ([\n",
        "T. ToTensor () ,\n",
        "T. Grayscale () ,\n",
        "T. Normalize ( mean =(0.5 ,) , std=(0.5 ,) )\n",
        "])\n",
        "\n",
        "\n",
        "# downloading our data separately for both the train and test sets and apply the transformation (preprocessing) on it.\n",
        "train_set = CIFAR10 ( root = 'CIFAR10', train =True ,transform = train_transform , download = True )\n",
        "test_set = CIFAR10 ( root = 'CIFAR10', train =False , transform = test_transform , download = True )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca29733e785041f9b9dc1d84dbdd49f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR10/cifar-10-python.tar.gz to CIFAR10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68rsuqTgrrZ"
      },
      "source": [
        "# separate the validation and training set. validation set is used to tune \n",
        "# the hyper-parameters, e.g learning-rate, number of hidden layers, etc.\n",
        "# we do the separation before creating batches of the train and test set \n",
        "# 80 percent of the train_set will be kept as the train set and 20 percent will be used as the validation.\n",
        "train_set_size = int(len(train_set) * 0.8)\n",
        "valid_set_size = len(train_set) - train_set_size\n",
        "train_set, validation_set = torch.utils.data.random_split(train_set, [train_set_size, valid_set_size])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_2droWwgwfg"
      },
      "source": [
        "# divide our data into batches and shuffle them.\n",
        "batchsize = 32\n",
        "trainset = DataLoader(train_set, batch_size = batchsize, shuffle=True)\n",
        "testset = DataLoader(test_set, batch_size = batchsize)\n",
        "validationset = DataLoader(validation_set, batch_size = batchsize) \n",
        "\n",
        "# specify the classes\n",
        "classes = {'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', \n",
        "            'frog', 'horse', 'ship', 'truck'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LztZdV1g1iU"
      },
      "source": [
        "# this is our neural network!!\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_hidden_layers, hidden_layer_size):\n",
        "        super(NeuralNet, self).__init__() # initialize the super class \n",
        "        \n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.deep_nn = nn.Sequential()\n",
        "        \n",
        "        # creating the input and the hidden layers. the activation function used is relu.\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.deep_nn.add_module(f'ff{i}', nn.Linear(input_size, hidden_layer_size[i]))\n",
        "            # after creating the input layer, the value of input_size needs to change\n",
        "            input_size = hidden_layer_size[i] \n",
        "\n",
        "        self.deep_nn.add_module(f'classifier', nn.Linear(hidden_layer_size[num_hidden_layers-1], output_size))\n",
        "   \n",
        "    # passing of the data\n",
        "    def forward(self, data, activation_func):\n",
        "        # the activation function for the input and the hidden layers is relu\n",
        "        for i in range(self.num_hidden_layers):\n",
        "            if activation_func == 'relu':\n",
        "                data = F.relu(self.deep_nn[i](data))\n",
        "            elif activation_func == 'sigmoid':\n",
        "                data = F.sigmoid(self.deep_nn[i](data))\n",
        "            elif activation_func == 'Tanh':\n",
        "                data = F.tanh(self.deep_nn[i](data))\n",
        "        # the activation function for the output layer is softmax but we aren't going to initialize that now\n",
        "        # because the loss function crossEntropyLoss already applies softmax.\n",
        "        \n",
        "        return self.deep_nn[self.num_hidden_layers](data)\n",
        "    \n",
        "    # loss function is used to measure how well the model (neural network) is doing\n",
        "    # On the other hand, the optimizer tries to adjust the weights and biases in such a way to minimize the loss function.\n",
        "    def training_the_model(self, learning_rate, activation_func, trainset, validationset):\n",
        "        # we chose Adam as our optimizer.\n",
        "        # the first argument we passed in is the list of parameters that we want the optimizer to work on.\n",
        "        # e.g if we want out optimizer to only adjust some weights and not the others, we can control that here.\n",
        "        # the second argument specifies the learning rate.\n",
        "        optimizer = optim.Adam(self.parameters(), learning_rate)\n",
        "\n",
        "        # we will keep updating the best loss while training the model. this will be used to check if we have arrived to the point of overfitting\n",
        "        bestLoss = 20000\n",
        "\n",
        "        # choosing our loss function\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # we will continue training the model until the validation loss starts increasing. epoch 200 is a maximum value of epochs we assume\n",
        "        for epoch in range(250):\n",
        "            # data is a batch of featuresets and labels\n",
        "            for data in trainset: \n",
        "                # here we are separating the featureset and labels\n",
        "                X, y = data\n",
        "                # pass our input through the neural network\n",
        "                # view(-1) flattens a tensor in PyTorch = brings all the rows one after another\n",
        "                output = self.forward(X.view(-1, 32*32), activation_func)\n",
        "                # calculate the loss by comparing the model's predicted guess and the actual label\n",
        "                # the crossEntropyLoss was the loss function that we were required to use.\n",
        "                l = loss(output, y)\n",
        "                optimizer.zero_grad()\n",
        "                l.backward()\n",
        "                # this will adjust the weights\n",
        "                # step() makes the optimizer iterate over all parameters (tensors) it is supposed to update \n",
        "                # and use their internally stored grad to update their values.\n",
        "                optimizer.step() \n",
        "\n",
        "            # this will prevent overfitting\n",
        "            validation_accuracy, validation_loss = self.evaluatingModel(validationset, activation_func)\n",
        "            if validation_loss < bestLoss: # this would imply overfitting\n",
        "              bestLoss = validation_loss\n",
        "            else:\n",
        "              return (epoch + 1) # return the number of epochs that were needed before the network starts to overfit.\n",
        "            \n",
        "    def evaluatingModel(self, validationset, activation_funct):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        lossTotal = 0\n",
        "\n",
        "        # choosing our loss function\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # we don't want to update our weights while evaluating our model\n",
        "        with torch.no_grad():\n",
        "            # data is a batch of featuresets and labels\n",
        "            for data in validationset:\n",
        "                X, y = data\n",
        "                output = self.forward(X.view(-1, 32*32), activation_funct)\n",
        "                # here we are evaluating our model. basically, comparing the models guess to the actual y value\n",
        "                for idx, i in enumerate(output):\n",
        "                    if torch.argmax(i) == y[idx]:\n",
        "                        correct += 1 # increment correct if the model has guessed correctly\n",
        "                    total += 1\n",
        "                # find the loss for each batch and add it to the total loss.\n",
        "                lossTotal += loss(self.forward(X.view(-1, 32*32), activation_funct), y)\n",
        "\n",
        "        #print(\"Accuracy: \", round(correct/total, 3))\n",
        "        # to get the average loss, divide the total loss by the number of batches\n",
        "        loss = lossTotal / len(validationset)\n",
        "        #print(\"Loss: \", loss)\n",
        "        return round(correct/total, 3), loss\n",
        "            \n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL4EHCWpg-hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a509b911-b89c-4e35-9a62-9dd2da05ba02"
      },
      "source": [
        "# sanity check\n",
        "# This is used to see the value of the accuracy and loss on the untrained network and compare it to my guess.\n",
        "correct = 0\n",
        "total = 0\n",
        "lossTotal = 0\n",
        "\n",
        "# choosing our loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "net = NeuralNet(32*32, 10, 2, [684, 344])\n",
        "\n",
        "with torch.no_grad():\n",
        "    # data is a batch of featuresets and labels\n",
        "    for data in trainset:\n",
        "        X, y = data\n",
        "        output = net.forward(X.view(-1, 32*32), 'relu')\n",
        "        # here we are evaluating our model. basically, comparing the models guess to the actual y value\n",
        "        for idx, i in enumerate(output):\n",
        "            if torch.argmax(i) == y[idx]:\n",
        "                correct += 1 # increment correct if the model has guessed correctly\n",
        "            total += 1\n",
        "        # find the loss for each batch and add it to the total loss.\n",
        "        lossTotal += loss(net.forward(X.view(-1, 32*32), 'relu'), y)\n",
        "        \n",
        "print(\"Accuracy: \", round(correct/total, 3))\n",
        "# to get the average loss, divide the total loss by the number of batches\n",
        "loss = lossTotal / len(trainset)\n",
        "print(\"Loss: \", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.102\n",
            "Loss:  tensor(2.3040)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ay4aTAXhEvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d192415d-700d-4c21-9795-729267481ba2"
      },
      "source": [
        "# in this cell, we will do grid search for the hyper-parameters\n",
        "# in the neuralNet we can change the # of hidden layers, # of neurons\n",
        "# this dictionary will store the value of the validation_accuracy and validation_loss based on the hyper-parameters\n",
        "grid_dictionary = {}\n",
        "# initialization of the hyper-parameters\n",
        "amount_of_neurons = [[512], [684, 344], [768, 512, 256], [819, 614, 409, 204]] \n",
        "activation_functions = ['relu', 'sigmoid']\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "# for loop to iterate over every combination of the hyper-parameters\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for activation_func in activation_functions:\n",
        "        for no_layer in range(4):\n",
        "            net = NeuralNet(32*32, 10, no_layer+1, amount_of_neurons[no_layer])\n",
        "            epoch = net.training_the_model(learning_rate, activation_func, trainset, validationset)\n",
        "            validation_accuracy, validation_loss = net.evaluatingModel(validationset, activation_func) \n",
        "            print(\"\\n \\n\")\n",
        "            print(\"epoch no: \", epoch, \"  learning rate: \", learning_rate, \"  activation_func: \", activation_func)\n",
        "            print(\"number of layer: \", no_layer + 1, \"  validation accuracy: \", validation_accuracy, \"  validation loss: \", validation_loss)\n",
        "            grid_dictionary[epoch, no_layer+1, activation_func, learning_rate] = [validation_accuracy, validation_loss]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            "\n",
            "epoch no:  3   learning rate:  0.1   activation_func:  relu\n",
            "number of layer:  1   validation accuracy:  0.176   validation loss:  tensor(147.7524)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  3   learning rate:  0.1   activation_func:  relu\n",
            "number of layer:  2   validation accuracy:  0.098   validation loss:  tensor(2.3258)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  4   learning rate:  0.1   activation_func:  relu\n",
            "number of layer:  3   validation accuracy:  0.102   validation loss:  tensor(2.3179)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.1   activation_func:  relu\n",
            "number of layer:  4   validation accuracy:  0.099   validation loss:  tensor(2.3111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            "\n",
            "epoch no:  4   learning rate:  0.1   activation_func:  sigmoid\n",
            "number of layer:  1   validation accuracy:  0.174   validation loss:  tensor(8.3740)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  3   learning rate:  0.1   activation_func:  sigmoid\n",
            "number of layer:  2   validation accuracy:  0.102   validation loss:  tensor(3.8583)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.1   activation_func:  sigmoid\n",
            "number of layer:  3   validation accuracy:  0.102   validation loss:  tensor(3.7556)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  3   learning rate:  0.1   activation_func:  sigmoid\n",
            "number of layer:  4   validation accuracy:  0.102   validation loss:  tensor(2.9972)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  relu\n",
            "number of layer:  1   validation accuracy:  0.262   validation loss:  tensor(3.1704)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  relu\n",
            "number of layer:  2   validation accuracy:  0.138   validation loss:  tensor(2.2119)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  4   learning rate:  0.01   activation_func:  relu\n",
            "number of layer:  3   validation accuracy:  0.142   validation loss:  tensor(2.2635)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  relu\n",
            "number of layer:  4   validation accuracy:  0.166   validation loss:  tensor(2.1923)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  sigmoid\n",
            "number of layer:  1   validation accuracy:  0.241   validation loss:  tensor(2.3933)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  sigmoid\n",
            "number of layer:  2   validation accuracy:  0.229   validation loss:  tensor(2.1025)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  4   learning rate:  0.01   activation_func:  sigmoid\n",
            "number of layer:  3   validation accuracy:  0.149   validation loss:  tensor(2.2439)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  2   learning rate:  0.01   activation_func:  sigmoid\n",
            "number of layer:  4   validation accuracy:  0.1   validation loss:  tensor(2.3052)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  3   learning rate:  0.001   activation_func:  relu\n",
            "number of layer:  1   validation accuracy:  0.384   validation loss:  tensor(1.7973)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  4   learning rate:  0.001   activation_func:  relu\n",
            "number of layer:  2   validation accuracy:  0.426   validation loss:  tensor(1.6862)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  5   learning rate:  0.001   activation_func:  relu\n",
            "number of layer:  3   validation accuracy:  0.423   validation loss:  tensor(1.6812)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  5   learning rate:  0.001   activation_func:  relu\n",
            "number of layer:  4   validation accuracy:  0.419   validation loss:  tensor(1.6835)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  8   learning rate:  0.001   activation_func:  sigmoid\n",
            "number of layer:  1   validation accuracy:  0.4   validation loss:  tensor(1.7458)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  9   learning rate:  0.001   activation_func:  sigmoid\n",
            "number of layer:  2   validation accuracy:  0.415   validation loss:  tensor(1.7089)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  8   learning rate:  0.001   activation_func:  sigmoid\n",
            "number of layer:  3   validation accuracy:  0.412   validation loss:  tensor(1.7216)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  9   learning rate:  0.001   activation_func:  sigmoid\n",
            "number of layer:  4   validation accuracy:  0.392   validation loss:  tensor(1.7589)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  18   learning rate:  0.0001   activation_func:  relu\n",
            "number of layer:  1   validation accuracy:  0.453   validation loss:  tensor(1.6004)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  9   learning rate:  0.0001   activation_func:  relu\n",
            "number of layer:  2   validation accuracy:  0.446   validation loss:  tensor(1.6153)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  5   learning rate:  0.0001   activation_func:  relu\n",
            "number of layer:  3   validation accuracy:  0.44   validation loss:  tensor(1.6351)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  6   learning rate:  0.0001   activation_func:  relu\n",
            "number of layer:  4   validation accuracy:  0.447   validation loss:  tensor(1.6034)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  48   learning rate:  0.0001   activation_func:  sigmoid\n",
            "number of layer:  1   validation accuracy:  0.395   validation loss:  tensor(1.7463)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  19   learning rate:  0.0001   activation_func:  sigmoid\n",
            "number of layer:  2   validation accuracy:  0.383   validation loss:  tensor(1.7486)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  21   learning rate:  0.0001   activation_func:  sigmoid\n",
            "number of layer:  3   validation accuracy:  0.386   validation loss:  tensor(1.7388)\n",
            "\n",
            " \n",
            "\n",
            "epoch no:  17   learning rate:  0.0001   activation_func:  sigmoid\n",
            "number of layer:  4   validation accuracy:  0.36   validation loss:  tensor(1.7915)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vprz9Rj84gd",
        "outputId": "ca0d5442-92a9-4639-e573-25ecfa9f06eb"
      },
      "source": [
        "# finding the training and validation loss of the best three configuration:\n",
        "# first best\n",
        "net = NeuralNet(32*32, 10, 4, [819, 614, 409, 204])\n",
        "epoch = net.training_the_model(0.0001, 'relu', trainset, validationset)\n",
        "validation_accuracy, validation_loss = net.evaluatingModel(validationset, 'relu') \n",
        "training_accuracy, training_loss = net.evaluatingModel(trainset, 'relu')\n",
        "test_accuracy, test_loss = net.evaluatingModel(testset, 'relu')\n",
        "\n",
        "print(\"For the best 1st model\")\n",
        "print(\"Validation loss: \", validation_loss)\n",
        "print(\"Training loss: \", training_loss)\n",
        "print(\"Test accuracy: \", test_accuracy)\n",
        "\n",
        "\n",
        "#second best\n",
        "net = NeuralNet(32*32, 10, 1, [512])\n",
        "epoch = net.training_the_model(0.0001, 'relu', trainset, validationset)\n",
        "validation_accuracy, validation_loss = net.evaluatingModel(validationset, 'relu') \n",
        "training_accuracy, training_loss = net.evaluatingModel(trainset, 'relu')\n",
        "test_accuracy, test_loss = net.evaluatingModel(testset, 'relu')\n",
        "\n",
        "print(\"\\n \\n\")\n",
        "print(\"For the best 2nd model\")\n",
        "print(\"Validation loss: \", validation_loss)\n",
        "print(\"Training loss: \", training_loss)\n",
        "print(\"Test accuracy: \", test_accuracy)\n",
        "\n",
        "#third best\n",
        "net = NeuralNet(32*32, 10, 2, [684, 344])\n",
        "epoch = net.training_the_model(0.0001, 'relu', trainset, validationset)\n",
        "validation_accuracy, validation_loss = net.evaluatingModel(validationset, 'relu') \n",
        "training_accuracy, training_loss = net.evaluatingModel(trainset, 'relu')\n",
        "test_accuracy, test_loss = net.evaluatingModel(testset, 'relu')\n",
        "\n",
        "print(\"\\n \\n\")\n",
        "print(\"For the best 3rd model\")\n",
        "print(\"Validation loss: \", validation_loss)\n",
        "print(\"Training loss: \", training_loss)\n",
        "print(\"Test accuracy: \", test_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the best 1st model\n",
            "Validation loss:  tensor(1.6107)\n",
            "Training loss:  tensor(1.2404)\n",
            "Test accuracy:  0.45\n",
            "\n",
            " \n",
            "\n",
            "For the best 2nd model\n",
            "Validation loss:  tensor(1.6166)\n",
            "Training loss:  tensor(1.2563)\n",
            "Test accuracy:  0.449\n",
            "\n",
            " \n",
            "\n",
            "For the best 3rd model\n",
            "Validation loss:  tensor(1.6074)\n",
            "Training loss:  tensor(1.2320)\n",
            "Test accuracy:  0.454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awEMQRqy1IHs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxjkbJ8h_byB",
        "outputId": "65fd6a36-7032-4e6b-b450-28c15518f9a5"
      },
      "source": [
        "# printing out the configurations\n",
        "print(\"Epoch   ||   # of hidden layers   ||   activation function   ||   learning rate   ||   validation accuracy   ||   validation loss\")\n",
        "for key, value in grid_dictionary.items():\n",
        "    validation_accuracy, validation_loss = value\n",
        "\n",
        "    validation_loss = float(validation_loss)\n",
        "    formatted_float = \"{:.2f}\".format(validation_loss)\n",
        "\n",
        "    learning_rate = float(key[3])\n",
        "    formatted_float = \"{:.2f}\".format(learning_rate)\n",
        "\n",
        "    print(key[0], \"%20.3f\" % key[1], \"%25s\" % key[2], \"%25s\" % key[3], \"%27f\" % validation_accuracy, \"%25s\" % formatted_float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(3, 1, 'relu', 0.1): [0.176, tensor(147.7524)], (3, 2, 'relu', 0.1): [0.098, tensor(2.3258)], (4, 3, 'relu', 0.1): [0.102, tensor(2.3179)], (2, 4, 'relu', 0.1): [0.099, tensor(2.3111)], (4, 1, 'sigmoid', 0.1): [0.174, tensor(8.3740)], (3, 2, 'sigmoid', 0.1): [0.102, tensor(3.8583)], (2, 3, 'sigmoid', 0.1): [0.102, tensor(3.7556)], (3, 4, 'sigmoid', 0.1): [0.102, tensor(2.9972)], (2, 1, 'relu', 0.01): [0.262, tensor(3.1704)], (2, 2, 'relu', 0.01): [0.138, tensor(2.2119)], (4, 3, 'relu', 0.01): [0.142, tensor(2.2635)], (2, 4, 'relu', 0.01): [0.166, tensor(2.1923)], (2, 1, 'sigmoid', 0.01): [0.241, tensor(2.3933)], (2, 2, 'sigmoid', 0.01): [0.229, tensor(2.1025)], (4, 3, 'sigmoid', 0.01): [0.149, tensor(2.2439)], (2, 4, 'sigmoid', 0.01): [0.1, tensor(2.3052)], (3, 1, 'relu', 0.001): [0.384, tensor(1.7973)], (4, 2, 'relu', 0.001): [0.426, tensor(1.6862)], (5, 3, 'relu', 0.001): [0.423, tensor(1.6812)], (5, 4, 'relu', 0.001): [0.419, tensor(1.6835)], (8, 1, 'sigmoid', 0.001): [0.4, tensor(1.7458)], (9, 2, 'sigmoid', 0.001): [0.415, tensor(1.7089)], (8, 3, 'sigmoid', 0.001): [0.412, tensor(1.7216)], (9, 4, 'sigmoid', 0.001): [0.392, tensor(1.7589)], (18, 1, 'relu', 0.0001): [0.453, tensor(1.6004)], (9, 2, 'relu', 0.0001): [0.446, tensor(1.6153)], (5, 3, 'relu', 0.0001): [0.44, tensor(1.6351)], (6, 4, 'relu', 0.0001): [0.447, tensor(1.6034)], (48, 1, 'sigmoid', 0.0001): [0.395, tensor(1.7463)], (19, 2, 'sigmoid', 0.0001): [0.383, tensor(1.7486)], (21, 3, 'sigmoid', 0.0001): [0.386, tensor(1.7388)], (17, 4, 'sigmoid', 0.0001): [0.36, tensor(1.7915)]}\n",
            "-------  ------  -------  -------  -------  -------  -------  -------  -------  -------  ------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------\n",
            "  0.176  0.098   0.102    0.099    0.174    0.102    0.102    0.102    0.262    0.138    0.142   0.166    0.241    0.229    0.149    0.1      0.384    0.426    0.423    0.419    0.4      0.415    0.412    0.392    0.453    0.446    0.44     0.447    0.395    0.383    0.386    0.36\n",
            "147.752  2.3258  2.31787  2.31106  8.37401  3.85827  3.75556  2.99718  3.17043  2.21191  2.2635  2.19234  2.39332  2.10254  2.24391  2.30524  1.79727  1.68617  1.68125  1.68353  1.74585  1.70894  1.72164  1.75893  1.60042  1.61531  1.63512  1.60339  1.74633  1.74863  1.73879  1.79146\n",
            "-------  ------  -------  -------  -------  -------  -------  -------  -------  -------  ------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------\n",
            "    5\n",
            "Epoch   ||   # of hidden layers   ||   activation function   ||   learning rate   ||   validation accuracy   ||   validation loss\n",
            "3                1.000                      relu                       0.1                    0.176000                      0.10\n",
            "3                2.000                      relu                       0.1                    0.098000                      0.10\n",
            "4                3.000                      relu                       0.1                    0.102000                      0.10\n",
            "2                4.000                      relu                       0.1                    0.099000                      0.10\n",
            "4                1.000                   sigmoid                       0.1                    0.174000                      0.10\n",
            "3                2.000                   sigmoid                       0.1                    0.102000                      0.10\n",
            "2                3.000                   sigmoid                       0.1                    0.102000                      0.10\n",
            "3                4.000                   sigmoid                       0.1                    0.102000                      0.10\n",
            "2                1.000                      relu                      0.01                    0.262000                      0.01\n",
            "2                2.000                      relu                      0.01                    0.138000                      0.01\n",
            "4                3.000                      relu                      0.01                    0.142000                      0.01\n",
            "2                4.000                      relu                      0.01                    0.166000                      0.01\n",
            "2                1.000                   sigmoid                      0.01                    0.241000                      0.01\n",
            "2                2.000                   sigmoid                      0.01                    0.229000                      0.01\n",
            "4                3.000                   sigmoid                      0.01                    0.149000                      0.01\n",
            "2                4.000                   sigmoid                      0.01                    0.100000                      0.01\n",
            "3                1.000                      relu                     0.001                    0.384000                      0.00\n",
            "4                2.000                      relu                     0.001                    0.426000                      0.00\n",
            "5                3.000                      relu                     0.001                    0.423000                      0.00\n",
            "5                4.000                      relu                     0.001                    0.419000                      0.00\n",
            "8                1.000                   sigmoid                     0.001                    0.400000                      0.00\n",
            "9                2.000                   sigmoid                     0.001                    0.415000                      0.00\n",
            "8                3.000                   sigmoid                     0.001                    0.412000                      0.00\n",
            "9                4.000                   sigmoid                     0.001                    0.392000                      0.00\n",
            "18                1.000                      relu                    0.0001                    0.453000                      0.00\n",
            "9                2.000                      relu                    0.0001                    0.446000                      0.00\n",
            "5                3.000                      relu                    0.0001                    0.440000                      0.00\n",
            "6                4.000                      relu                    0.0001                    0.447000                      0.00\n",
            "48                1.000                   sigmoid                    0.0001                    0.395000                      0.00\n",
            "19                2.000                   sigmoid                    0.0001                    0.383000                      0.00\n",
            "21                3.000                   sigmoid                    0.0001                    0.386000                      0.00\n",
            "17                4.000                   sigmoid                    0.0001                    0.360000                      0.00\n"
          ]
        }
      ]
    }
  ]
}